<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="spectral-graph-convolutions---fourier-transforms-on-graphs-for-the-birdwatcher">Spectral Graph Convolutions - Fourier Transforms (on Graphs) for the Birdwatcher</h1>
<p>In the spirit of Francis Crick, who in trying to explain X-ray crystallography to a colleague, simplified the mathematical concepts such that he considered writing up his explanations under the title ‘Fourier Transforms for the Birdwatcher’ - this post will attempt to do the same with spectral graph convolutions.</p>
<h3 id="convolution">Convolution</h3>
<p>Before we jump into spectral convolutions on graphs, we need to understand how convolution works in the spatial and frequency domains. On grid structured data such as 1D signals and 2D images, we can conveniently perform convolutions directly in the spatial domain. This is achieved by passing a filter <span class="math inline">\(w\)</span> over each pixel <span class="math inline">\(h\)</span> and computing the inner product between the neighbourhood of each pixel and the filter values.</p>
<p><span class="math display">\[w * h = \sum_{j \in \mathcal{N}_{i}} \langle w_{j}, h_{i-j}\rangle \]</span></p>
<p>where <span class="math inline">\(\mathcal{N}_{i}\)</span> is the neighbourhood of pixel <span class="math inline">\(i\)</span>. This is possible because pixels exist in a grid with indices which define exactly where they exist on that grid. The vertices of a graph however are permutationally invariant and are not indexed in a way that encodes their structure. As a result, we can’t perform convolution directly like this. This forces us into the Fourier domain. The convolution theorem states that the Fourier transform of the convolution of two functions is the pointwise product of their Fourier transforms.</p>
<p><span class="math display">\[w * h = \mathcal{F}^{-1}(\mathcal{F(w)*\mathcal{F}(h)})\]</span></p>
<p>Thus we can obtain the convolution by first multiplying our functions in the Fourier domain, and subsequently returning via the inverse Fourier transform. Ok, but if <span class="math inline">\(h\)</span> is a node feature vector on a graph, how do you take the Fourier Transform of nodes on a graph?</p>
<h3 id="fourier-transform-of-a-graph">Fourier Transform of a Graph</h3>
<p>The Fourier transform of a real-valued function is obtained by multiplying by a set of Fourier basis vectors. Each of these basis vectors <span class="math inline">\(w_k \in \mathbb{C}^N\)</span> is a sinusoid with frequency <span class="math inline">\(\frac{2\pi k}{N}\)</span>. Multiplying a function by the matrix containing the Fourier basis vectors can be viewed from the lens of template matching or correlation. Each inner product <span class="math inline">\(\langle w_{k}, x\rangle\)</span>, measures the similarity between <span class="math inline">\(x\)</span> and the basis vector <span class="math inline">\(w_k\)</span> - how much does this function resemble a sinusoid of frequency <span class="math inline">\(\frac{2\pi k}{N}\)</span>. If we compute these inner products for all Fourier basis functions, we obtain the frequency-space representation of <span class="math inline">\(x\)</span>. So how do we do it for graphs? First, a definition that is key to understanding this…</p>
<h4 id="graph-laplacian">Graph Laplacian</h4>
<p>The (normalised) Laplacian of a graph is defined as</p>
<p><span class="math display">\[\Delta = I - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\]</span></p>
<p>where A is an adjacency matrix, such that <span class="math inline">\(A_{ij} = 1\)</span> if node <span class="math inline">\(i\)</span> is connected to node <span class="math inline">\(j\)</span>, and 0 otherwise; and D is the degree matrix - a diagonal matrix where <span class="math inline">\(D_{ii}\)</span> is the number of nodes connected to node <span class="math inline">\(i\)</span> and <span class="math inline">\(D_{ij} = 0 \:\: \forall \:\: (i, j), i \neq j\)</span>. Intuitively, the Laplacian is a measure of local smoothness in the graph. Multiplying a node signal <span class="math inline">\(h_i\)</span> by <span class="math inline">\(\Delta\)</span> represents how much a node differs from its neighbours.</p>
<p><span class="math display">\[\Delta h_i = h_i - \frac{1}{d_i}\sum_{j \in \mathcal{N}_i}A_{ij}h_j\]</span></p>
<p>If the signal is “smooth”, or the neighbourhood of node <span class="math inline">\(h_i\)</span> does not vary much from itself, this difference will be close to zero. However, if there is high variance in <span class="math inline">\(\mathcal{N}_i\)</span>, the difference will be large. Thus, the Laplacian is simply a measure of the smoothness of a function on a graph. What’s the point of the Laplacian?</p>
<p><strong>The eigenvectors of the Laplacian matrix of a graph are the Fourier basis vectors that we will use to take the Fourier transform of signals defined on that graph.</strong></p>
<p>By taking the eigendecomposition of <span class="math inline">\(\Delta\)</span>, we obtain the eigenvectors <span class="math inline">\(\Phi\)</span> and hence Fourier basis vectors.</p>
<p><span class="math display">\[\Delta = \Phi^T \Lambda \Phi\]</span></p>
<h4 id="spectral-graph-convolutions">Spectral Graph Convolutions</h4>
<p>Now we can take our graph signal <span class="math inline">\(h\)</span> into Fourier space as we would with a real-valued function:</p>
<p><span class="math display">\[\hat{h} = \mathcal{F}(h) = \Phi^Th\]</span></p>
<p>We do the same with the weights of the convolutional filter:</p>
<p><span class="math display">\[\hat{w} = \mathcal{F}(w) = \Phi^Tw\]</span></p>
<p>Just as we mentioned, to perform convolution in the frequency domain we take the pointwise product of <span class="math inline">\(\hat{h}\)</span> and <span class="math inline">\(\hat{w}\)</span>. Finally, we can obtain our desired convolution by inverse Fourier transforming back to the spatial domain.</p>
<p><span class="math display">\[w * h = \Phi(\hat{w} \odot \hat{h})\]</span></p>
<p>where multiplication by <span class="math inline">\(\Phi\)</span> constitutes the inverse Fourier transform.</p>
<p><em>Note: this can actually be simplified further with a bit of algebra. I’m going to leave it there however, since I feel that this illustrates what we’re trying to achieve in a way that’s super intuitive if you know a bit of signal processing/Fourier analysis.</em></p>
<h3 id="conclusion">Conclusion</h3>
<p>That’s it. The key here is that we’re able to obtain the Fourier basis vectors from the eigendecomposition of the graph Laplacian. By taking advantage of the convolution theorem, we can perform convolution on signals on graphs by taking a short detour into the frequency domain to multiply, and returning via the inverse Fourier transform to obtain the result we’re after.</p>
<p>A quick warning though. While this would actually be fairly straightforward to implement, in practice, this method is quite inefficient. Plenty of tricks have been discovered recently to increase the efficiency of spectral graph convs and it should be possible to uncover them with a quick search of the literature.</p>
<p>Here are a few resources that were useful in understanding spectral graph convolutions and in writing this:</p>
<p>[1] J. Bruna, W. Zaremba, A. Szlam, Y. LeCun. Spectral Networks and Deep Locally Connected Networks on Graphs. <em>arXiv:1312.6203</em> (2013)</p>
<p> [2] M. Henaff, J. Bruna, Y. LeCun. Deep Convolutional Networks on Graph-Structured Data. <em>arXiv:1506.05163</em>, (2015)</p>
<p> [3] M. Defferrard, X. Bresson, P. Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. <em>Advances in Neural Information Processing Systems</em> (2016)</p>
<p> [4] F. Chung. Spectral Graph Theory. <em>American Mathematical Society</em> (1997)</p>
<p>[5] X. Bresson. Lecture: Graph Convolutional Networks. <em>https://www.youtube.com/watch?v=Iiv9R6BjxHM</em> (2020)</p>
